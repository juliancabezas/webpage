---
title: Paralelizando R en Windows (usando el paquete parallel)
author: Julian Cabezas
date: '2019-01-07'
slug: paralelizando-r-en-windows-usando-el-paquete-parallel
categories:
  - R
  - Tutoriales
  - parallel
  - windows
  - parlapply
  - machine learning
  - SVM
  - tune
tags: []
image:
  caption: ''
  focal_point: ''
---



<div id="por-que-paralelizar" class="section level1">
<h1>¿Por que Paralelizar?</h1>
<p>En tu trabajo te pidieron correr un código que se demora 20 horas en ejecutarse, consiste en ejecutar la misma función con distintos parámetros o datos muchas pero <strong>muchas</strong> veces.</p>
<p>A pesar de que puede que para ti esto puede no ser un problema, es común que te pidan las cosas <del>para ayer</del> sin 20 horas de anticipación. Si este es tu caso, y te estás preguntando por que tu PC o workstation viene con 4, 8 o 16 núcleos ¡este tutorial es para ti!</p>
<p>Paralelizar procesos en R puede ser difícil, pero si a eso le sumas <del>la mala suerte</del> el hecho de estar trabajando en Windows, se puede convertir en una tarea casi imposible</p>
<p>En este tutorial (<strong>¡el primero de esta pagina!</strong>), vamos a realizar una pequeña introducción a la paralelización de procesos en R bajo el SO Windows, partiendo por el uso de la familia de funciones <strong>apply</strong>, el uso del paquete parallel en R y una solución para crear un cluster en windows, terminando con un ejemplo de aplicación muy común en <em>machine learning</em></p>
<div id="nivel-de-este-tutorial" class="section level4">
<h4>Nivel de este tutorial:</h4>
<p>Al escribir este tutorial estoy asumiendo que eres un usuario intermedio o avanzado de R.</p>
</div>
<div id="convertir-tu-loop-en-una-funcion-y-usar-lapply" class="section level2">
<h2>Convertir tu loop en una función y usar lapply</h2>
<p>En este ejemplo vamos a utilizar la base de datos <strong>iris</strong>, una de las más conocidas entre los usuarios de R, esta base de datos tiene mediciones del largo y ancho de pétalos y sépalos de 3 especies de flores</p>
<pre class="r"><code>data(iris)
knitr::kable(head(iris))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">Sepal.Length</th>
<th align="right">Sepal.Width</th>
<th align="right">Petal.Length</th>
<th align="right">Petal.Width</th>
<th align="left">Species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">5.1</td>
<td align="right">3.5</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="even">
<td align="right">4.9</td>
<td align="right">3.0</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="odd">
<td align="right">4.7</td>
<td align="right">3.2</td>
<td align="right">1.3</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="even">
<td align="right">4.6</td>
<td align="right">3.1</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="odd">
<td align="right">5.0</td>
<td align="right">3.6</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="even">
<td align="right">5.4</td>
<td align="right">3.9</td>
<td align="right">1.7</td>
<td align="right">0.4</td>
<td align="left">setosa</td>
</tr>
</tbody>
</table>
<p>A alguien le interesa generar un modelo lineal entre los pétalos y los sépalos. Debido a que la muestra es pequeña (150 individuos) te mandan a hacer un bootstrap de 20000 iteraciones para ver como se comporta el R<sup>2</sup> y los coeficientes del modelo.</p>
<p>Para poder lograr esto hacemos un loop, que hace lo siguiente: - Saca una muestra de 100 individuos (dos tercios del total) - Genera el modelo lineal - Guarda el R<sup>2</sup> y los coeficientes en una dataframe</p>
<pre class="r"><code>results &lt;- data.frame() #Hacemos una dataframe vacia para guardar los resultados

system.time({
for (i in 1:20000) {
  ind &lt;- sample(1:nrow(iris), size=100) # Saco una muestra de 100 numeros entre 1 y 150
  iris_sample &lt;- iris[ind,] # Saco la muestra de iris
  model1 &lt;- lm(iris_sample[,&quot;Sepal.Length&quot;]~iris_sample[,&quot;Petal.Length&quot;]) # Hago un lm con el petalo y el sepalo
  R2 &lt;- summary(model1)$r.squared # Extraigo el R2
  coef &lt;- coefficients(model1) # Extraigo los coeficientes
  results &lt;- rbind(results, c(R2,coef)) # lo junto todo en la data.frame de resultados
}
})</code></pre>
<pre><code>##    user  system elapsed 
##   20.70    0.02   20.72</code></pre>
<p>El algoritmo se demora un poco más de 20 segundos en funcionar, tal vez esto no signifique un problema, pero ¿que pasa si el día de mañana te piden hacer lo mismo con una base de datos de medio millón de observaciones?.</p>
<p>El loop ocupa un núcleo de tu procesador para ejecutar la operación una iteración tras otra, lo que no es muy eficiente</p>
<p>El primer paso para aprovechar los beneficios de la paralelización en convertir tu loop en una función, este caso el único argumento de nuestra función será un vector “iter”, que es el que nos dará la opción de realizar más iteraciones</p>
<pre class="r"><code>lm_boot_fun &lt;- function(iter) {
  ind &lt;- sample(1:nrow(iris),size=100) # Saco una muestra de 100 numeros entre 1 y 150
  iris_sample &lt;- iris[ind,] # Saco la muestra de iris
  model1 &lt;- lm(iris_sample[,&quot;Sepal.Length&quot;]~iris_sample[,&quot;Petal.Length&quot;]) # Hago un lm con el petalo y el sepalo
  R2 &lt;- summary(model1)$r.squared # Extraigo el R2
  coef &lt;- coefficients(model1) # Extraigo los coeficientes
  results &lt;- c(&quot;R2&quot;=R2,coef) # Creo un vector de resultados
  return(results) # El output de mi función
}</code></pre>
<p>Fácil ¿no?, bueno, este lo fue, en otros casos nos gustaría que los argumentos incluyeran la base de datos, el tamaño de la muestra y un largo etc. Pero lo estamos manteniendo simple</p>
<p>!Ahora probemos nuestra función!. Como podemos ver en el siguiente código, la función da resultados independiente del argumento que le pongamos</p>
<pre class="r"><code>result &lt;- lm_boot_fun(1)
result</code></pre>
<pre><code>##                            R2                   (Intercept) 
##                     0.7814819                     4.2478433 
## iris_sample[, &quot;Petal.Length&quot;] 
##                     0.4194369</code></pre>
<pre class="r"><code>result &lt;- lm_boot_fun(56876)
result</code></pre>
<pre><code>##                            R2                   (Intercept) 
##                     0.7626287                     4.2183015 
## iris_sample[, &quot;Petal.Length&quot;] 
##                     0.4303577</code></pre>
<p>Si le pusiéramos un vector nos daría solo valor, por que “results” se sobrescribe en cada iteración</p>
<pre class="r"><code>result &lt;- lm_boot_fun(1:10)
result</code></pre>
<pre><code>##                            R2                   (Intercept) 
##                     0.7834460                     4.2615107 
## iris_sample[, &quot;Petal.Length&quot;] 
##                     0.4258869</code></pre>
<p>¿Como hacemos para que nos guarde todos los resultados?: ¡Recurriendo a la función lapply!. La función lapply pertenece a la familia de funciones apply (apply, sapply, vapply, lapply y mapply), una de las herramientas más potentes de R (y que me habría gustado aprender mucho antes), este tutorial no se trata sobre ellas, por lo que solo ocuparemos lapply, esta función aplica otra función (como la que construimos), sobre una lista o vector de argumentos, y retorna una lista del mismo tamaño que los argumentos</p>
<pre class="r"><code>iter &lt;- seq(1, 20000)
system.time({
  results &lt;- lapply(iter, lm_boot_fun)
})</code></pre>
<pre><code>##    user  system elapsed 
##   14.28    0.00   14.28</code></pre>
<p>Bueno, como podemos ver, ya bajamos un poco los tiempos de procesamiento, pero queremos hacerlo mucho más rápido aún. El único problema es que la función lapply te da como resultado una lista</p>
<pre class="r"><code>head(results, n=3)</code></pre>
<pre><code>## [[1]]
##                            R2                   (Intercept) 
##                     0.7294631                     4.4070464 
## iris_sample[, &quot;Petal.Length&quot;] 
##                     0.3925767 
## 
## [[2]]
##                            R2                   (Intercept) 
##                      0.739758                      4.377762 
## iris_sample[, &quot;Petal.Length&quot;] 
##                      0.402413 
## 
## [[3]]
##                            R2                   (Intercept) 
##                     0.7828706                     4.2718067 
## iris_sample[, &quot;Petal.Length&quot;] 
##                     0.4224847</code></pre>
<p>Para convertir esta lista en una dataframe, utilizamos la función unlist para generar una matriz que se convierte en una dataframe. especificamos que queremos 3 columnas</p>
<pre class="r"><code>results &lt;- data.frame(matrix(unlist(results), ncol=3, byrow=T))
head(results, n=3)</code></pre>
<pre><code>##          X1       X2        X3
## 1 0.7294631 4.407046 0.3925767
## 2 0.7397580 4.377762 0.4024130
## 3 0.7828706 4.271807 0.4224847</code></pre>
<p>Otras soluciones más elegantes para este problema pueden ser encontradas en <a href="https://stackoverflow.com/questions/4227223/r-list-to-data-frame">este post de stackoverflow</a></p>
</div>
<div id="usando-el-paquete-parallel" class="section level2">
<h2>Usando el paquete parallel</h2>
<p>El paquete parallel, desde hace algunas versiones, viene incluido con R. Este increíble paquete contiene versiones en paralelo para todas las funciones de la familia apply, para utilizar esta función necesitamos saber primero con cuantos núcleos contamos</p>
<pre class="r"><code>library(parallel)
numCores &lt;- detectCores()
numCores</code></pre>
<pre><code>## [1] 8</code></pre>
<p>¡Tenemos 8 núcleos lógicos!. En este caso, como regla general, vamos a dejar un núcleo disponible para que el computador no se nos quede inservible. Si tuviéramos un sistema tipo Unix (linux o mac, por ejemplo), podríamos utilizar la función mclapply. Sin embargo, mclapply utiliza <em>forking</em>, una útil función que simplemente crea una copia del sistema y realiza la operación, solo disponible en sistemas tipo Unix</p>
<pre class="r"><code>iter &lt;- seq(1, 20000)
system.time({
  results &lt;- mclapply(iter, lm_boot_fun, mc.cores = 7)
})</code></pre>
<pre><code>## Error in mclapply(iter, lm_boot_fun, mc.cores = 7): &#39;mc.cores&#39; &gt; 1 is not supported on Windows</code></pre>
<pre><code>## Timing stopped at: 0 0 0</code></pre>
<p>En nuestro caso, lo que haremos es crear un cluster manualmente, para eso, utilizamos la función makeCluster, lo que creara un objeto que utilizaremos como argumento en la función parlapply</p>
<pre class="r"><code>cl &lt;- makeCluster(2) # Hacemos el cluster
system.time(
results&lt;-parLapply(cl,iter,lm_boot_fun)
)</code></pre>
<pre><code>##    user  system elapsed 
##    0.02    0.00    7.61</code></pre>
<pre class="r"><code>stopCluster(cl) # Cerramos el cluster, NO OLVIDAR</code></pre>
<p><strong>Wow</strong>, bajamos MUCHO el tiempo de procesamiento de nuestro bootstrap, ya podemos ir a tomarnos un matecito con tranquilidad, no sin antes destruir el cluster con la función stopCluster <strong>¡Bien!</strong> <img src="https://media.giphy.com/media/4xpB3eE00FfBm/giphy.gif" alt="¡BIEN!" /></p>
<p>Podríamos dejar este tutorial aquí, pero este ejemplo es muy simple, en general utilizamos bases de datos que tenemos en nuestro computador, y 5-10 librerías.</p>
<p>Probemos simulando que iris es una base de datos que tenemos en nuestra computador, adicionalmente, a nuestra función le agregaremos dos argumentos referidos a la base de datos y al tamaño de la muestra</p>
<pre class="r"><code>write.csv(iris,file=&quot;iris.csv&quot;)

datos_pc&lt;-read.csv(&quot;iris.csv&quot;)

lm_boot_fun2 &lt;- function(iter,data,n_sample) {
  ind &lt;- sample(1:nrow(data),size=n_sample) # Saco una muestra
  data_sample &lt;- data[ind,] # Saco la muestra de iris
  model1 &lt;- lm(data_sample[,&quot;Sepal.Length&quot;]~data_sample[,&quot;Petal.Length&quot;]) # Hago un lm con el petalo y el sepalo
  R2 &lt;- summary(model1)$r.squared # Extraigo el R2
  coef &lt;- coefficients(model1) # Extraigo los coeficientes
  results &lt;- c(&quot;R2&quot;=R2,coef) # Creo un vector de resultados
  return(results) # El output de mi función
}</code></pre>
<p>Ahora vamos a hacer lo mismo pero con los argumentos adicionales de parlapply</p>
<pre class="r"><code>cl &lt;- makeCluster(2) # Hacemos el cluster
system.time(
results&lt;-parLapply(cl,iter,lm_boot_fun2,data=datos_pc,n_sample=100)
)</code></pre>
<pre><code>##    user  system elapsed 
##    0.01    0.00    7.80</code></pre>
<pre class="r"><code>stopCluster(cl) # Cerramos el cluster, NO OLVIDAR</code></pre>
<p>Ahora pensemos que entra nuestro jefe diciendo que el cliente quiere un Random Forest (probablemente por que suena bacán/chevere), adaptemos nuestra función complacer a todos, vamos a usar la librería randomForest, y a guardar el pseudo-R2 y el MSE</p>
<pre class="r"><code>library(randomForest)


rf_boot_fun &lt;- function(iter,data,n_sample) {
  ind &lt;- sample(1:nrow(data),size=n_sample) # Saco una muestra de numeros
  data_sample &lt;- data[ind,] # Saco la muestra
  model1 &lt;- randomForest(Sepal.Length~Petal.Length,data=data_sample) # Hago un RF con el petalo y el sepalo
  R2 &lt;- model1$rsq[length(model1$rsq)]  # Extraigo el pseudo-R2
  MSE &lt;- model1$mse[length(model1$mse)]  # Extraigo el MSE
  results &lt;- c(&quot;R2&quot;=R2,&quot;MSE&quot;=MSE) # Creo un vector de resultados
  return(results) # El output de mi función
}</code></pre>
<p>Ahora usemos parLapply</p>
<pre class="r"><code>library(randomForest)
cl &lt;- makeCluster(4) # Hacemos el cluster
system.time(
results&lt;-parLapply(cl,iter,rf_boot_fun,data=datos_pc,n_sample=100)
)</code></pre>
<pre><code>## Error in checkForRemoteErrors(val): 4 nodes produced errors; first error: no se pudo encontrar la función &quot;randomForest&quot;</code></pre>
<pre><code>## Timing stopped at: 0 0 0</code></pre>
<pre class="r"><code>stopCluster(cl) # Cerramos el cluster, NO OLVIDAR</code></pre>
<p><strong>¿¿QUE PASÓ??</strong> se preguntarán, ¡ya llamé a library(randomForest)!</p>
<div class="figure">
<img src="https://media.giphy.com/media/DVkZv0SltX1h6/giphy.gif" alt="QUEEEE" />
<p class="caption">QUEEEE</p>
</div>
<p>Lo que pasó es lo siguiente: ya tenemos cargado el paquete randomForest en nuestra sesión, sin embargo, en el momento que ocupamos la función makeCluster, se abren <em>n</em> nuevas sesiones de R, cada una desde cero, por lo que lo que debemos hacer es cargar los paquetes en todas estas sesiones, en algunos casos, también es necesario exportar las bases de datos y funciones.</p>
<p>Para cargar el paquete randomForest en los nodos del cluster ocupamos la función clusterCall, que nos permite ejecutar una función en todos los clusters, en este caso la utilizamos para cargar la librería randomForest en todos los cluster</p>
<pre class="r"><code>cl &lt;- makeCluster(4) # Hacemos el cluster
clusterCall(cl, function() library(randomForest))</code></pre>
<pre><code>## [[1]]
## [1] &quot;randomForest&quot; &quot;stats&quot;        &quot;graphics&quot;     &quot;grDevices&quot;   
## [5] &quot;utils&quot;        &quot;datasets&quot;     &quot;methods&quot;      &quot;base&quot;        
## 
## [[2]]
## [1] &quot;randomForest&quot; &quot;stats&quot;        &quot;graphics&quot;     &quot;grDevices&quot;   
## [5] &quot;utils&quot;        &quot;datasets&quot;     &quot;methods&quot;      &quot;base&quot;        
## 
## [[3]]
## [1] &quot;randomForest&quot; &quot;stats&quot;        &quot;graphics&quot;     &quot;grDevices&quot;   
## [5] &quot;utils&quot;        &quot;datasets&quot;     &quot;methods&quot;      &quot;base&quot;        
## 
## [[4]]
## [1] &quot;randomForest&quot; &quot;stats&quot;        &quot;graphics&quot;     &quot;grDevices&quot;   
## [5] &quot;utils&quot;        &quot;datasets&quot;     &quot;methods&quot;      &quot;base&quot;</code></pre>
<pre class="r"><code>system.time(
results&lt;-parLapply(cl,iter,rf_boot_fun,data=datos_pc,n_sample=100)
)</code></pre>
<pre><code>##    user  system elapsed 
##       0       0     137</code></pre>
<pre class="r"><code>stopCluster(cl) # Cerramos el cluster, NO OLVIDAR</code></pre>
</div>
<div id="un-ejemplo-mas-realista-usando-machine-learning" class="section level2">
<h2>Un ejemplo más realista (usando Machine Learning)</h2>
<p>Ok, mucho jugar y poca acción, vamos a presentar un ejemplo más realista, en este caso, el <em>tuning</em> de un modelo. “tunear” un modelo se refiere muchas veces a escoger el valor optimo de uno o más parámetros, de acuerdo a algún estadístico de rendimiento del modelo (como el R<sup>2</sup>o el RMSE), este tuneo se hace en la base de datos de entrenamiento o calibración, y el modelo se valida en los datos de validación.</p>
<p><strong>Tunear un modelo siempre me ha sonado como a “enchular” un auto</strong> <img src="https://media.giphy.com/media/xmf00ANvBCTzG/giphy.gif" /></p>
<p>Vamos a simular que la base de datos “iris” es nuestra base de datos de entrenamiento, vamos a suponer que solo tenemos información sobre los sépalos (para hacerlo un poco más difícil)</p>
<p>En este caso utilizaremos un Support Vector Machine (SVM) para intentar predecir la especie en base al largo y ancho de los pétalos y sépalos, este modelo es muy bueno y funciona genial para tareas de clasificación, sin embargo, a ratos es bastante dependiente del parámetro de costo y el parámetro de gamma (en el caso que utilicemos la función radial), vamos a intentar optimizar ambos parámetros</p>
<p>En este caso, el paquete <strong>e1071</strong> ya trae una función “tune”, que permite usar <em>cross validation</em> para tunear el parámetro. En este caso se utilizaremos <em>10-fold</em> con 3 repeticiones, pero podríamos ocupar un <em>bootstrap</em> por ejemplo. Este ejemplo lo saco de un caso real de un trabajo, en el que debíamos construir un modelo con una matriz de datos gigante, cada ejecución del modelo se tardaba aproximadamente 1 hora y media en ejecutarse, ahora imagínense las 30 ejecuciones de la cross validation propuesta…</p>
<p>¡Basta de hablar, veamos el modelo!</p>
<pre class="r"><code>library(e1071)
svm_model &lt;- svm(Species ~ Sepal.Length+Sepal.Width, data=iris)
summary(svm_model)</code></pre>
<pre><code>## 
## Call:
## svm(formula = Species ~ Sepal.Length + Sepal.Width, data = iris)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  1 
##       gamma:  0.5 
## 
## Number of Support Vectors:  86
## 
##  ( 10 40 36 )
## 
## 
## Number of Classes:  3 
## 
## Levels: 
##  setosa versicolor virginica</code></pre>
<p>Como podemos ver, por defecto el costo que se utiliza es 1 y el gamma 0.5, veamos algo de las <em>performances</em></p>
<pre class="r"><code>library(caret)</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre><code>## 
## Attaching package: &#39;ggplot2&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:randomForest&#39;:
## 
##     margin</code></pre>
<pre class="r"><code>confusionMatrix(iris$Species, predict(svm_model))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         49          1         0
##   versicolor      0         36        14
##   virginica       0         12        38
## 
## Overall Statistics
##                                          
##                Accuracy : 0.82           
##                  95% CI : (0.749, 0.8779)
##     No Information Rate : 0.3467         
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16      
##                                          
##                   Kappa : 0.73           
##  Mcnemar&#39;s Test P-Value : NA             
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.7347           0.7308
## Specificity                 0.9901            0.8614           0.8776
## Pos Pred Value              0.9800            0.7200           0.7600
## Neg Pred Value              1.0000            0.8700           0.8600
## Prevalence                  0.3267            0.3267           0.3467
## Detection Rate              0.3267            0.2400           0.2533
## Detection Prevalence        0.3333            0.3333           0.3333
## Balanced Accuracy           0.9950            0.7980           0.8042</code></pre>
<p>Tenemos un <em>Accuracy</em> de 0.82, en muchos casos eso no es suficiente, tratemos de mejorarlo tuneando los parámetros</p>
<p>Creamos una función que con los argumentos costo y gamma me de el rendimiento del modelo, como se puede observar, solo utilizo la función “tune” con un parámetro a la vez, se podría utilizar con una lista de parámetros, pero sería más difícil de paralelizar</p>
<pre class="r"><code>tune_svm_par&lt;- function (costo,gamma) {

  control&lt;-tune.control(nrepeat = 3, repeat.aggregate = mean,
                        sampling = &quot;cross&quot;, sampling.aggregate = mean,
                        sampling.dispersion = sd,
                        cross = 10)
  
  svm_cv &lt;- tune(&quot;svm&quot;, train.x =  iris[,c(&quot;Sepal.Length&quot;,&quot;Sepal.Width&quot;)],
                 train.y = iris$Species,
                 kernel = &quot;radial&quot;, 
                 scale = TRUE,
                 type = &quot;C-classification&quot;,
                 tunecontrol = control,
                 ranges = list(cost = costo,gamma=gamma))

return(svm_cv$performances)
}</code></pre>
<p>Y ahora, para paralelizarla, vamos a tener que utilizar la función “clusterMap”, que nos permite poner más de un argumento variable (es el equivalente a “mapply”)</p>
<pre class="r"><code>costos.test&lt;-seq(0.2,4,by=0.1)
gammas.test&lt;-seq(0.2,4,by=0.1)

grilla&lt;-expand.grid(&quot;costos&quot;=costos.test,&quot;gammas&quot;=gammas.test)
nrow(grilla)</code></pre>
<pre><code>## [1] 1521</code></pre>
<pre class="r"><code>t1&lt;-Sys.time()

cl &lt;- makeCluster(4) # Hacemos el cluster
clusterCall(cl, function() library(e1071))</code></pre>
<pre><code>## [[1]]
## [1] &quot;e1071&quot;     &quot;stats&quot;     &quot;graphics&quot;  &quot;grDevices&quot; &quot;utils&quot;     &quot;datasets&quot; 
## [7] &quot;methods&quot;   &quot;base&quot;     
## 
## [[2]]
## [1] &quot;e1071&quot;     &quot;stats&quot;     &quot;graphics&quot;  &quot;grDevices&quot; &quot;utils&quot;     &quot;datasets&quot; 
## [7] &quot;methods&quot;   &quot;base&quot;     
## 
## [[3]]
## [1] &quot;e1071&quot;     &quot;stats&quot;     &quot;graphics&quot;  &quot;grDevices&quot; &quot;utils&quot;     &quot;datasets&quot; 
## [7] &quot;methods&quot;   &quot;base&quot;     
## 
## [[4]]
## [1] &quot;e1071&quot;     &quot;stats&quot;     &quot;graphics&quot;  &quot;grDevices&quot; &quot;utils&quot;     &quot;datasets&quot; 
## [7] &quot;methods&quot;   &quot;base&quot;</code></pre>
<pre class="r"><code>list.errores = clusterMap(cl=cl,fun=tune_svm_par,
                    costo= grilla$costos, 
                    gamma= grilla$gammas,
                    SIMPLIFY = FALSE,RECYCLE=FALSE)

library(dplyr)
errores.bind&lt;-bind_rows(list.errores)

stopCluster(cl)

t2&lt;-Sys.time()

print(t2-t1)</code></pre>
<pre><code>## Time difference of 54.40942 secs</code></pre>
<p>Vamos a buscar la combinación de costo y gamma que nos da un menor error</p>
<pre class="r"><code>knitr::kable(subset(errores.bind,errores.bind$error==min(errores.bind$error)))</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">cost</th>
<th align="right">gamma</th>
<th align="right">error</th>
<th align="right">dispersion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>855</td>
<td align="right">3.7</td>
<td align="right">2.3</td>
<td align="right">0.18</td>
<td align="right">0.0945424</td>
</tr>
</tbody>
</table>
<p>Ahora probemos nuestro svm con esos parámetros</p>
<pre class="r"><code>library(e1071)
svm_model &lt;- svm(Species ~ Sepal.Length+Sepal.Width, data=iris,cost=2.6,gamma=2)
library(caret)
confusionMatrix(iris$Species, predict(svm_model))</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         50          0         0
##   versicolor      0         36        14
##   virginica       0         10        40
## 
## Overall Statistics
##                                           
##                Accuracy : 0.84            
##                  95% CI : (0.7714, 0.8947)
##     No Information Rate : 0.36            
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16       
##                                           
##                   Kappa : 0.76            
##  Mcnemar&#39;s Test P-Value : NA              
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                 1.0000            0.7826           0.7407
## Specificity                 1.0000            0.8654           0.8958
## Pos Pred Value              1.0000            0.7200           0.8000
## Neg Pred Value              1.0000            0.9000           0.8600
## Prevalence                  0.3333            0.3067           0.3600
## Detection Rate              0.3333            0.2400           0.2667
## Detection Prevalence        0.3333            0.3333           0.3333
## Balanced Accuracy           1.0000            0.8240           0.8183</code></pre>
<p>Como podemos observar, en este caso logramos subir la accuracy en el entrenamiento de 0.82 a 0.84, esto no parece tanto, pero un 2% de incremento por un procedimiento que nos tardó 55 segundos (¡gracias a la paralelización!) es bastante bueno</p>
<p><strong>¡Muchas gracias por llegar hasta el final de este tutorial!</strong> <strong>Espero sus comentariós y dudas</strong></p>
</div>
</div>
